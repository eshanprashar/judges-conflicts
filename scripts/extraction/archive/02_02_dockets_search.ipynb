{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import pyarrow.parquet as pq\n",
    "from utils import api_token\n",
    "import requests\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eshan23/eshanprashar_git_profile/judges-conflicts/data/persons_positions/people_positions_disc_summary.parquet\n"
     ]
    }
   ],
   "source": [
    "# Defining the file path for importing\n",
    "current_path = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_path)\n",
    "grandparent_dir = os.path.dirname(parent_dir)\n",
    "data_dir = os.path.join(grandparent_dir, 'data')\n",
    "file_path = os.path.join(data_dir, 'persons_positions/people_positions_disc_summary.parquet')\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_id</th>\n",
       "      <th>judge_flag</th>\n",
       "      <th>position</th>\n",
       "      <th>status</th>\n",
       "      <th>disclosures_count</th>\n",
       "      <th>name_first</th>\n",
       "      <th>name_middle</th>\n",
       "      <th>name_last</th>\n",
       "      <th>political_affiliations</th>\n",
       "      <th>race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>non-judge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>George</td>\n",
       "      <td>None</td>\n",
       "      <td>Washington</td>\n",
       "      <td>f</td>\n",
       "      <td>w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>non-judge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John</td>\n",
       "      <td>None</td>\n",
       "      <td>Adams</td>\n",
       "      <td>f</td>\n",
       "      <td>w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>non-judge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thomas</td>\n",
       "      <td>None</td>\n",
       "      <td>Jefferson</td>\n",
       "      <td>j</td>\n",
       "      <td>w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>non-judge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>James</td>\n",
       "      <td>None</td>\n",
       "      <td>Madison</td>\n",
       "      <td>j</td>\n",
       "      <td>w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>non-judge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>James</td>\n",
       "      <td>None</td>\n",
       "      <td>Monroe</td>\n",
       "      <td>j</td>\n",
       "      <td>w</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   person_id  judge_flag  position     status  disclosures_count name_first  \\\n",
       "0          1           0         1  non-judge                NaN     George   \n",
       "1          2           0         1  non-judge                NaN       John   \n",
       "2          3           0         1  non-judge                NaN     Thomas   \n",
       "3          4           0         1  non-judge                NaN      James   \n",
       "4          5           0         1  non-judge                NaN      James   \n",
       "\n",
       "  name_middle   name_last political_affiliations race  \n",
       "0        None  Washington                      f    w  \n",
       "1        None       Adams                      f    w  \n",
       "2        None   Jefferson                      j    w  \n",
       "3        None     Madison                      j    w  \n",
       "4        None      Monroe                      j    w  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import and examine the data\n",
    "people_fin_dis_summary = pd.read_parquet(file_path)\n",
    "people_fin_dis_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_id</th>\n",
       "      <th>judge_flag</th>\n",
       "      <th>position</th>\n",
       "      <th>status</th>\n",
       "      <th>disclosures_count</th>\n",
       "      <th>name_first</th>\n",
       "      <th>name_middle</th>\n",
       "      <th>name_last</th>\n",
       "      <th>political_affiliations</th>\n",
       "      <th>race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>judge with multiple positions</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Leslie</td>\n",
       "      <td>Joyce</td>\n",
       "      <td>Abrams</td>\n",
       "      <td>d</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>judge with multiple positions</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Ronnie</td>\n",
       "      <td>None</td>\n",
       "      <td>Abrams</td>\n",
       "      <td>d</td>\n",
       "      <td>w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>judge with multiple positions</td>\n",
       "      <td>14.0</td>\n",
       "      <td>William</td>\n",
       "      <td>Marsh</td>\n",
       "      <td>Acker</td>\n",
       "      <td>r</td>\n",
       "      <td>w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>judge with multiple positions</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Harold</td>\n",
       "      <td>Arnold</td>\n",
       "      <td>Ackerman</td>\n",
       "      <td>d</td>\n",
       "      <td>w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>judge with multiple positions</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Raymond</td>\n",
       "      <td>L.</td>\n",
       "      <td>Acosta</td>\n",
       "      <td>r</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    person_id  judge_flag  position                         status  \\\n",
       "43         44           1         6  judge with multiple positions   \n",
       "44         45           1         9  judge with multiple positions   \n",
       "47         48           1         3  judge with multiple positions   \n",
       "48         49           7        10  judge with multiple positions   \n",
       "50         51           1         6  judge with multiple positions   \n",
       "\n",
       "    disclosures_count name_first name_middle name_last political_affiliations  \\\n",
       "43                5.0     Leslie       Joyce    Abrams                      d   \n",
       "44                9.0     Ronnie        None    Abrams                      d   \n",
       "47               14.0    William       Marsh     Acker                      r   \n",
       "48                6.0     Harold      Arnold  Ackerman                      d   \n",
       "50                7.0    Raymond          L.    Acosta                      r   \n",
       "\n",
       "   race  \n",
       "43    b  \n",
       "44    w  \n",
       "47    w  \n",
       "48    w  \n",
       "50    h  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new dataframe where disclosures_count is atleast 1 and 'non-judge' values from status are excluded\n",
    "judges_with_disclosures_summary = people_fin_dis_summary[(people_fin_dis_summary['disclosures_count'] >= 1) & (people_fin_dis_summary['status'] != 'non-judge')]\n",
    "judges_with_disclosures_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this to a csv\n",
    "judges_with_disclosures_summary.to_csv(os.path.join(data_dir, 'dockets/intermediate_dfs/judges_with_disclosures_summary.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_id</th>\n",
       "      <th>judge_flag</th>\n",
       "      <th>position</th>\n",
       "      <th>status</th>\n",
       "      <th>disclosures_count</th>\n",
       "      <th>name_first</th>\n",
       "      <th>name_middle</th>\n",
       "      <th>name_last</th>\n",
       "      <th>political_affiliations</th>\n",
       "      <th>race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2783</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>judge with multiple positions</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Lee</td>\n",
       "      <td>Hyman</td>\n",
       "      <td>Rosenthal</td>\n",
       "      <td>r</td>\n",
       "      <td>w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3163</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>judge with multiple positions</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Richard</td>\n",
       "      <td>C.</td>\n",
       "      <td>Tallman</td>\n",
       "      <td>d</td>\n",
       "      <td>w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9090</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>judge with 1 position</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Robert</td>\n",
       "      <td>M.</td>\n",
       "      <td>Illman</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2034</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>judge with multiple positions</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Algenon</td>\n",
       "      <td>L.</td>\n",
       "      <td>Marbley</td>\n",
       "      <td>d</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>686</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>judge with multiple positions</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Steven</td>\n",
       "      <td>M.</td>\n",
       "      <td>Colloton</td>\n",
       "      <td>r</td>\n",
       "      <td>w</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   person_id  judge_flag  position                         status  \\\n",
       "0       2783           1         3  judge with multiple positions   \n",
       "1       3163           1         5  judge with multiple positions   \n",
       "2       9090           1         1          judge with 1 position   \n",
       "3       2034           1         4  judge with multiple positions   \n",
       "4        686           1         9  judge with multiple positions   \n",
       "\n",
       "   disclosures_count name_first name_middle  name_last political_affiliations  \\\n",
       "0               11.0        Lee       Hyman  Rosenthal                      r   \n",
       "1               16.0    Richard          C.    Tallman                      d   \n",
       "2                4.0     Robert          M.     Illman                   None   \n",
       "3               16.0    Algenon          L.    Marbley                      d   \n",
       "4               18.0     Steven          M.   Colloton                      r   \n",
       "\n",
       "   race  \n",
       "0     w  \n",
       "1     w  \n",
       "2  None  \n",
       "3     b  \n",
       "4     w  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now constructing an api request with a sample of 25 judges\n",
    "seed = 0\n",
    "sample_size = 25\n",
    "sampled_judges_with_disclosures = judges_with_disclosures_summary.sample(n=sample_size, random_state=seed).reset_index(drop=True)\n",
    "sampled_judges_with_disclosures.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT Prompt:\n",
    "I have a dataframe called \"sampled_judges_with_disclosures\" with a column \"person_id\" - I want to go through the person_id one by one and fetch data for each using the api request. For each, I want to save the data to a csv and log errors. Once requests for all authors are done, I want to collate results from all csvs into a single dataframe. Please construct functions to achieve this workflow.\n",
    "\n",
    "### Response:\n",
    "To achieve your goal of fetching data for each judge with financial disclosures from CourtListener's API and then saving the results to CSV files, you can construct a workflow consisting of the following steps:\n",
    "\n",
    "Function to Fetch Data from the API: This function will take an author_id (equivalent to the person_id in your dataframe) and fetch the data using the API.\n",
    "Function to Save Data to CSV: This function will save the data to a CSV file named after the author_id.\n",
    "Main Function to Iterate Over All Judges: This function will iterate over all person_id values in your dataframe, fetch the data, save it, and handle errors.\n",
    "Function to Combine All CSVs into a Single DataFrame: This function will read all CSVs and combine them into one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(author_id, api_token):\n",
    "    \"\"\"\n",
    "    Fetch data for a given author_id using CourtListener API with pagination.\n",
    "    \"\"\"\n",
    "    base_url = f\"https://www.courtlistener.com/api/rest/v4/search/\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Token {api_token}\"\n",
    "    }\n",
    "    \n",
    "    # Start with an initial page size\n",
    "    initial_page_size = 100  \n",
    "    params = {\n",
    "        \"q\": f\"author_id:{author_id}\",\n",
    "        \"page_size\": initial_page_size\n",
    "    }\n",
    "\n",
    "    all_results = []\n",
    "    next_page = base_url\n",
    "    total_count = None\n",
    "\n",
    "    while next_page:\n",
    "        try:\n",
    "            response = requests.get(next_page, headers=headers, params=params)\n",
    "            response.raise_for_status()  # Raise an error for non-200 responses\n",
    "            data = response.json()\n",
    "\n",
    "            if total_count is None:\n",
    "                total_count = data.get('count', 0)  # Get total count from the first response\n",
    "\n",
    "            results = data.get('results', [])\n",
    "            if results:\n",
    "                all_results.extend(results)  # Append results from the current page\n",
    "\n",
    "            next_page = data.get('next')  # Get the next page URL\n",
    "\n",
    "            # Dynamically adjust page size based on response time or rate limits\n",
    "            if len(all_results) < total_count // 2:\n",
    "                params['page_size'] = min(params['page_size'] * 2, 1000)  # Increase page_size up to 1000\n",
    "            else:\n",
    "                params['page_size'] = max(params['page_size'] // 2, 50)  # Decrease page_size down to 50 if too high\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            log_error(author_id, str(e))\n",
    "            return None\n",
    "\n",
    "    # Check if the total count matches the expected count\n",
    "    if len(all_results) != total_count:\n",
    "        log_error(author_id, f\"Mismatch in total count. Expected: {total_count}, Fetched: {len(all_results)}\")\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to log errors to a JSON file\n",
    "def log_error(author_id, error_message, log_file='api_errors.json'):\n",
    "    \"\"\"\n",
    "    Log errors to a JSON file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(log_file):\n",
    "        with open(log_file, 'w') as f:\n",
    "            json.dump([], f)  # Initialize the file with an empty list\n",
    "    \n",
    "    with open(log_file, 'r+') as f:\n",
    "        errors = json.load(f)\n",
    "        errors.append({\"author_id\": author_id, \"error\": error_message})\n",
    "        f.seek(0)\n",
    "        json.dump(errors, f, indent=4)\n",
    "    print(f\"Error logged for author_id {author_id}: {error_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save data to CSV\n",
    "def save_to_csv(data, author_id, output_dir='api_results'):\n",
    "    \"\"\"\n",
    "    Save the fetched data to a CSV file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    if data:\n",
    "        df = pd.json_normalize(data)  # Normalize JSON data to a DataFrame\n",
    "        csv_filename = os.path.join(output_dir, f\"{author_id}.csv\")\n",
    "        df.to_csv(csv_filename, index=False)\n",
    "        print(f\"Data saved to {csv_filename}\")\n",
    "    else:\n",
    "        log_error(author_id, \"No data to save.\")\n",
    "        print(f\"No data to save for author_id {author_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to iterate over the sampled judges and fetch data\n",
    "def fetch_and_save_all_judges(df, api_token, output_dir='api_results'):\n",
    "    \"\"\"\n",
    "    Fetch and save data for all judges in the dataframe.\n",
    "    \"\"\"\n",
    "    for i, author_id in enumerate(df['person_id']):\n",
    "        print(f\"Fetching data for author_id: {author_id} ({i+1}/{len(df)})\")\n",
    "        data = fetch_data(author_id, api_token)\n",
    "        save_to_csv(data, author_id, output_dir=output_dir)\n",
    "        time.sleep(1)  # Sleep to avoid hitting the API rate limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine all CSVs into a single dataframe\n",
    "def combine_csvs_to_dataframe(output_dir='api_results'):\n",
    "    \"\"\"\n",
    "    Combine all CSV files in the output directory into a single DataFrame.\n",
    "    \"\"\"\n",
    "    all_files = glob.glob(os.path.join(output_dir, \"*.csv\"))\n",
    "    print(f\"Found {len(all_files)} files in the directory\")\n",
    "    li = []\n",
    "    for filename in all_files:\n",
    "        try:\n",
    "            df = pd.read_csv(filename, index_col=None, header=0)\n",
    "            li.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {filename}: {e}\")\n",
    "    return pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a URL that will work\n",
    "url = \"https://www.courtlistener.com/api/rest/v4/search/?q=author_id:48\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for author_id: 44 (1/3345)\n",
      "Error logged for author_id 44: No data to save.\n",
      "No data to save for author_id 44\n",
      "Fetching data for author_id: 45 (2/3345)\n",
      "Error logged for author_id 45: No data to save.\n",
      "No data to save for author_id 45\n",
      "Fetching data for author_id: 48 (3/3345)\n",
      "Data saved to api_results/48.csv\n",
      "Fetching data for author_id: 49 (4/3345)\n",
      "Data saved to api_results/49.csv\n",
      "Fetching data for author_id: 51 (5/3345)\n",
      "Data saved to api_results/51.csv\n",
      "Fetching data for author_id: 57 (6/3345)\n",
      "Data saved to api_results/57.csv\n",
      "Fetching data for author_id: 58 (7/3345)\n",
      "Data saved to api_results/58.csv\n",
      "Fetching data for author_id: 60 (8/3345)\n",
      "Data saved to api_results/60.csv\n",
      "Fetching data for author_id: 63 (9/3345)\n",
      "Data saved to api_results/63.csv\n",
      "Fetching data for author_id: 64 (10/3345)\n",
      "Data saved to api_results/64.csv\n",
      "Fetching data for author_id: 66 (11/3345)\n",
      "Data saved to api_results/66.csv\n",
      "Fetching data for author_id: 69 (12/3345)\n",
      "Data saved to api_results/69.csv\n",
      "Fetching data for author_id: 70 (13/3345)\n",
      "Error logged for author_id 70: No data to save.\n",
      "No data to save for author_id 70\n",
      "Fetching data for author_id: 71 (14/3345)\n",
      "Data saved to api_results/71.csv\n",
      "Fetching data for author_id: 72 (15/3345)\n",
      "Error logged for author_id 72: No data to save.\n",
      "No data to save for author_id 72\n",
      "Fetching data for author_id: 73 (16/3345)\n",
      "Data saved to api_results/73.csv\n",
      "Fetching data for author_id: 77 (17/3345)\n",
      "Data saved to api_results/77.csv\n",
      "Fetching data for author_id: 78 (18/3345)\n",
      "Error logged for author_id 78: No data to save.\n",
      "No data to save for author_id 78\n",
      "Fetching data for author_id: 86 (19/3345)\n",
      "Error logged for author_id 86: No data to save.\n",
      "No data to save for author_id 86\n",
      "Fetching data for author_id: 88 (20/3345)\n",
      "Data saved to api_results/88.csv\n",
      "Fetching data for author_id: 89 (21/3345)\n",
      "Data saved to api_results/89.csv\n",
      "Fetching data for author_id: 91 (22/3345)\n",
      "Data saved to api_results/91.csv\n",
      "Fetching data for author_id: 92 (23/3345)\n",
      "Data saved to api_results/92.csv\n",
      "Fetching data for author_id: 94 (24/3345)\n",
      "Error logged for author_id 94: No data to save.\n",
      "No data to save for author_id 94\n",
      "Fetching data for author_id: 95 (25/3345)\n",
      "Data saved to api_results/95.csv\n",
      "Fetching data for author_id: 97 (26/3345)\n",
      "Data saved to api_results/97.csv\n",
      "Fetching data for author_id: 98 (27/3345)\n",
      "Data saved to api_results/98.csv\n",
      "Fetching data for author_id: 101 (28/3345)\n",
      "Data saved to api_results/101.csv\n",
      "Fetching data for author_id: 105 (29/3345)\n",
      "Error logged for author_id 105: No data to save.\n",
      "No data to save for author_id 105\n",
      "Fetching data for author_id: 106 (30/3345)\n",
      "Data saved to api_results/106.csv\n",
      "Fetching data for author_id: 107 (31/3345)\n",
      "Data saved to api_results/107.csv\n",
      "Fetching data for author_id: 109 (32/3345)\n",
      "Data saved to api_results/109.csv\n",
      "Fetching data for author_id: 110 (33/3345)\n",
      "Error logged for author_id 110: No data to save.\n",
      "No data to save for author_id 110\n",
      "Fetching data for author_id: 113 (34/3345)\n",
      "Error logged for author_id 113: No data to save.\n",
      "No data to save for author_id 113\n",
      "Fetching data for author_id: 114 (35/3345)\n",
      "Data saved to api_results/114.csv\n",
      "Fetching data for author_id: 116 (36/3345)\n",
      "Data saved to api_results/116.csv\n",
      "Fetching data for author_id: 117 (37/3345)\n",
      "Data saved to api_results/117.csv\n",
      "Fetching data for author_id: 119 (38/3345)\n",
      "Data saved to api_results/119.csv\n",
      "Fetching data for author_id: 122 (39/3345)\n",
      "Error logged for author_id 122: No data to save.\n",
      "No data to save for author_id 122\n",
      "Fetching data for author_id: 123 (40/3345)\n",
      "Data saved to api_results/123.csv\n",
      "Fetching data for author_id: 124 (41/3345)\n",
      "Error logged for author_id 124: No data to save.\n",
      "No data to save for author_id 124\n",
      "Fetching data for author_id: 125 (42/3345)\n",
      "Data saved to api_results/125.csv\n",
      "Fetching data for author_id: 126 (43/3345)\n",
      "Data saved to api_results/126.csv\n",
      "Fetching data for author_id: 127 (44/3345)\n",
      "Data saved to api_results/127.csv\n",
      "Fetching data for author_id: 128 (45/3345)\n",
      "Data saved to api_results/128.csv\n",
      "Fetching data for author_id: 133 (46/3345)\n",
      "Data saved to api_results/133.csv\n",
      "Fetching data for author_id: 134 (47/3345)\n",
      "Data saved to api_results/134.csv\n",
      "Fetching data for author_id: 137 (48/3345)\n",
      "Data saved to api_results/137.csv\n",
      "Fetching data for author_id: 141 (49/3345)\n",
      "Data saved to api_results/141.csv\n",
      "Fetching data for author_id: 143 (50/3345)\n",
      "Data saved to api_results/143.csv\n",
      "Fetching data for author_id: 144 (51/3345)\n",
      "Data saved to api_results/144.csv\n",
      "Fetching data for author_id: 145 (52/3345)\n",
      "Data saved to api_results/145.csv\n",
      "Fetching data for author_id: 146 (53/3345)\n",
      "Error logged for author_id 146: No data to save.\n",
      "No data to save for author_id 146\n",
      "Fetching data for author_id: 147 (54/3345)\n",
      "Data saved to api_results/147.csv\n",
      "Fetching data for author_id: 148 (55/3345)\n",
      "Data saved to api_results/148.csv\n",
      "Fetching data for author_id: 150 (56/3345)\n",
      "Data saved to api_results/150.csv\n",
      "Fetching data for author_id: 152 (57/3345)\n",
      "Data saved to api_results/152.csv\n",
      "Fetching data for author_id: 154 (58/3345)\n",
      "Error logged for author_id 154: No data to save.\n",
      "No data to save for author_id 154\n",
      "Fetching data for author_id: 156 (59/3345)\n",
      "Data saved to api_results/156.csv\n",
      "Fetching data for author_id: 164 (60/3345)\n",
      "Data saved to api_results/164.csv\n",
      "Fetching data for author_id: 166 (61/3345)\n",
      "Data saved to api_results/166.csv\n",
      "Fetching data for author_id: 168 (62/3345)\n",
      "Data saved to api_results/168.csv\n",
      "Fetching data for author_id: 170 (63/3345)\n",
      "Data saved to api_results/170.csv\n",
      "Fetching data for author_id: 172 (64/3345)\n",
      "Data saved to api_results/172.csv\n",
      "Fetching data for author_id: 174 (65/3345)\n",
      "Error logged for author_id 174: No data to save.\n",
      "No data to save for author_id 174\n",
      "Fetching data for author_id: 178 (66/3345)\n",
      "Data saved to api_results/178.csv\n",
      "Fetching data for author_id: 181 (67/3345)\n",
      "Data saved to api_results/181.csv\n",
      "Fetching data for author_id: 184 (68/3345)\n",
      "Error logged for author_id 184: No data to save.\n",
      "No data to save for author_id 184\n",
      "Fetching data for author_id: 185 (69/3345)\n",
      "Data saved to api_results/185.csv\n",
      "Fetching data for author_id: 187 (70/3345)\n",
      "Error logged for author_id 187: No data to save.\n",
      "No data to save for author_id 187\n",
      "Fetching data for author_id: 189 (71/3345)\n",
      "Data saved to api_results/189.csv\n",
      "Fetching data for author_id: 191 (72/3345)\n",
      "Data saved to api_results/191.csv\n",
      "Fetching data for author_id: 193 (73/3345)\n",
      "Data saved to api_results/193.csv\n",
      "Fetching data for author_id: 194 (74/3345)\n",
      "Error logged for author_id 194: No data to save.\n",
      "No data to save for author_id 194\n",
      "Fetching data for author_id: 196 (75/3345)\n",
      "Data saved to api_results/196.csv\n",
      "Fetching data for author_id: 197 (76/3345)\n",
      "Error logged for author_id 197: No data to save.\n",
      "No data to save for author_id 197\n",
      "Fetching data for author_id: 199 (77/3345)\n",
      "Data saved to api_results/199.csv\n",
      "Fetching data for author_id: 200 (78/3345)\n",
      "Data saved to api_results/200.csv\n",
      "Fetching data for author_id: 201 (79/3345)\n",
      "Data saved to api_results/201.csv\n",
      "Fetching data for author_id: 202 (80/3345)\n",
      "Data saved to api_results/202.csv\n",
      "Fetching data for author_id: 203 (81/3345)\n",
      "Data saved to api_results/203.csv\n",
      "Fetching data for author_id: 204 (82/3345)\n",
      "Data saved to api_results/204.csv\n",
      "Fetching data for author_id: 205 (83/3345)\n",
      "Data saved to api_results/205.csv\n",
      "Fetching data for author_id: 208 (84/3345)\n",
      "Data saved to api_results/208.csv\n",
      "Fetching data for author_id: 210 (85/3345)\n",
      "Data saved to api_results/210.csv\n",
      "Fetching data for author_id: 213 (86/3345)\n",
      "Data saved to api_results/213.csv\n",
      "Fetching data for author_id: 215 (87/3345)\n",
      "Data saved to api_results/215.csv\n",
      "Fetching data for author_id: 216 (88/3345)\n",
      "Data saved to api_results/216.csv\n",
      "Fetching data for author_id: 222 (89/3345)\n",
      "Data saved to api_results/222.csv\n",
      "Fetching data for author_id: 226 (90/3345)\n",
      "Data saved to api_results/226.csv\n",
      "Fetching data for author_id: 228 (91/3345)\n",
      "Data saved to api_results/228.csv\n",
      "Fetching data for author_id: 233 (92/3345)\n",
      "Data saved to api_results/233.csv\n",
      "Fetching data for author_id: 234 (93/3345)\n",
      "Error logged for author_id 234: No data to save.\n",
      "No data to save for author_id 234\n",
      "Fetching data for author_id: 235 (94/3345)\n",
      "Data saved to api_results/235.csv\n",
      "Fetching data for author_id: 236 (95/3345)\n",
      "Data saved to api_results/236.csv\n",
      "Fetching data for author_id: 241 (96/3345)\n",
      "Data saved to api_results/241.csv\n",
      "Fetching data for author_id: 245 (97/3345)\n",
      "Data saved to api_results/245.csv\n",
      "Fetching data for author_id: 246 (98/3345)\n",
      "Error logged for author_id 246: No data to save.\n",
      "No data to save for author_id 246\n",
      "Fetching data for author_id: 247 (99/3345)\n",
      "Data saved to api_results/247.csv\n",
      "Fetching data for author_id: 249 (100/3345)\n",
      "Data saved to api_results/249.csv\n",
      "Fetching data for author_id: 250 (101/3345)\n",
      "Error logged for author_id 250: No data to save.\n",
      "No data to save for author_id 250\n",
      "Fetching data for author_id: 252 (102/3345)\n",
      "Data saved to api_results/252.csv\n",
      "Fetching data for author_id: 253 (103/3345)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Fetch data for all judges and save to CSVs\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m fetch_and_save_all_judges(judges_with_disclosures_summary, api_token)\n",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m, in \u001b[0;36mfetch_and_save_all_judges\u001b[0;34m(df, api_token, output_dir)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m i, author_id \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(df[\u001b[39m'\u001b[39m\u001b[39mperson_id\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m      7\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFetching data for author_id: \u001b[39m\u001b[39m{\u001b[39;00mauthor_id\u001b[39m}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(df)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     data \u001b[39m=\u001b[39m fetch_data(author_id, api_token)\n\u001b[1;32m      9\u001b[0m     save_to_csv(data, author_id, output_dir\u001b[39m=\u001b[39moutput_dir)\n\u001b[1;32m     10\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 23\u001b[0m, in \u001b[0;36mfetch_data\u001b[0;34m(author_id, api_token)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mwhile\u001b[39;00m next_page:\n\u001b[1;32m     22\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m         response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(next_page, headers\u001b[39m=\u001b[39;49mheaders, params\u001b[39m=\u001b[39;49mparams)\n\u001b[1;32m     24\u001b[0m         response\u001b[39m.\u001b[39mraise_for_status()  \u001b[39m# Raise an error for non-200 responses\u001b[39;00m\n\u001b[1;32m     25\u001b[0m         data \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/eshanprashar_git_profile/judges-conflicts/.venv/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/eshanprashar_git_profile/judges-conflicts/.venv/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/eshanprashar_git_profile/judges-conflicts/.venv/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/eshanprashar_git_profile/judges-conflicts/.venv/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/eshanprashar_git_profile/judges-conflicts/.venv/lib/python3.11/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    668\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    669\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    670\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    671\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    672\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    673\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    674\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    675\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    676\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    677\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    678\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    681\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/eshanprashar_git_profile/judges-conflicts/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    790\u001b[0m     conn,\n\u001b[1;32m    791\u001b[0m     method,\n\u001b[1;32m    792\u001b[0m     url,\n\u001b[1;32m    793\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    794\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    795\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    796\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    797\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    798\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    799\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    800\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    801\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    802\u001b[0m )\n\u001b[1;32m    804\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/eshanprashar_git_profile/judges-conflicts/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[39m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    537\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/eshanprashar_git_profile/judges-conflicts/.venv/lib/python3.11/site-packages/urllib3/connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresponse\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    463\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    466\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     assert_header_parsing(httplib_response\u001b[39m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1379\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline(_MAXLINE \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fetch data for all judges and save to CSVs\n",
    "fetch_and_save_all_judges(judges_with_disclosures_summary, api_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 105 files in the directory\n",
      "Error reading file api_results/1211.csv: No columns to parse from file\n",
      "Error reading file api_results/234.csv: No columns to parse from file\n",
      "Error reading file api_results/8781.csv: No columns to parse from file\n",
      "Error reading file api_results/8888.csv: No columns to parse from file\n",
      "Error reading file api_results/9629.csv: No columns to parse from file\n",
      "Error reading file api_results/2145.csv: No columns to parse from file\n",
      "Error reading file api_results/9207.csv: No columns to parse from file\n",
      "Error reading file api_results/2955.csv: No columns to parse from file\n",
      "Error reading file api_results/9433.csv: No columns to parse from file\n",
      "Error reading file api_results/10144.csv: No columns to parse from file\n",
      "Error reading file api_results/8884.csv: No columns to parse from file\n",
      "Error reading file api_results/8893.csv: No columns to parse from file\n",
      "Error reading file api_results/9391.csv: No columns to parse from file\n",
      "Error reading file api_results/9082.csv: No columns to parse from file\n",
      "Error reading file api_results/9640.csv: No columns to parse from file\n",
      "Error reading file api_results/9090.csv: No columns to parse from file\n",
      "                                        absolute_url  \\\n",
      "0  /opinion/1466032/lm-ex-rel-sam-m-v-capistrano-...   \n",
      "1          /opinion/1449182/carrillo-jaime-v-holder/   \n",
      "2                   /opinion/1447792/renee-v-duncan/   \n",
      "3  /opinion/1442458/lm-ex-rel-sam-m-v-capistrano-...   \n",
      "4                    /opinion/1436690/khan-v-holder/   \n",
      "\n",
      "                                            attorney  \\\n",
      "0  S. Daniel Harbottle, Rutan & Tucker, LLP, Cost...   \n",
      "1  Kari Elisabeth Hong, Portland, OR, for the pet...   \n",
      "2  John T. Affeldt and Tara Kini, Public Advocate...   \n",
      "3  S. Daniel Harbottle, Rutan & Tucker, LLP, Cost...   \n",
      "4  Robert Bradford Jobe, Law Offices of Robert B....   \n",
      "\n",
      "                                            caseName  \\\n",
      "0  L.M. Ex Rel. Sam M. v. Capistrano Unified Scho...   \n",
      "1                           Carrillo-Jaime v. Holder   \n",
      "2                                    Renee v. Duncan   \n",
      "3  L.M. Ex Rel. Sam M. v. Capistrano Unified Scho...   \n",
      "4                                     Khan v. Holder   \n",
      "\n",
      "                                        caseNameFull  \\\n",
      "0  L.M., a Minor by and Through His Guardian Ad L...   \n",
      "1  Reinaldo Otoniel CARRILLO-JAIME, AKA Reinaldo ...   \n",
      "2  Sonya RENEE; Candice Johnson, a Minor, by Sony...   \n",
      "3  L.M., a Minor by and Through His Guardian Ad L...   \n",
      "4  Anjam Parvez KHAN, Petitioner, v. Eric H. HOLD...   \n",
      "\n",
      "                                            citation  citeCount  cluster_id  \\\n",
      "0                 ['556 F.3d 900', '2009 WL 349795']         23     1466032   \n",
      "1  ['572 F.3d 747', '2009 U.S. App. LEXIS 15591',...         18     1449182   \n",
      "2  ['573 F.3d 903', '2009 U.S. App. LEXIS 16300',...          5     1447792   \n",
      "3               ['538 F.3d 1261', '2008 WL 3843465']          2     1442458   \n",
      "4  ['584 F.3d 773', '2009 U.S. App. LEXIS 20134',...        123     1436690   \n",
      "\n",
      "                                    court court_citation_string court_id  ...  \\\n",
      "0  Court of Appeals for the Ninth Circuit              9th Cir.      ca9  ...   \n",
      "1  Court of Appeals for the Ninth Circuit              9th Cir.      ca9  ...   \n",
      "2  Court of Appeals for the Ninth Circuit              9th Cir.      ca9  ...   \n",
      "3  Court of Appeals for the Ninth Circuit              9th Cir.      ca9  ...   \n",
      "4  Court of Appeals for the Ninth Circuit              9th Cir.      ca9  ...   \n",
      "\n",
      "  posture procedural_history  scdb_id                  sibling_ids source  \\\n",
      "0     NaN                NaN      NaN                    [1466032]     LU   \n",
      "1     NaN                NaN      NaN  [9632491, 9632492, 1449182]     LU   \n",
      "2     NaN                NaN      NaN  [1447792, 9632207, 9632208]     LU   \n",
      "3     NaN                NaN      NaN                    [1442458]     LU   \n",
      "4     NaN                NaN      NaN  [9857061, 9857062, 1436690]     LU   \n",
      "\n",
      "      status suitNature syllabus               meta.timestamp  \\\n",
      "0  Published        NaN      NaN  2024-06-21T04:02:32.009336Z   \n",
      "1  Published        NaN      NaN  2024-06-21T03:56:25.933357Z   \n",
      "2  Published        NaN      NaN  2024-06-21T03:56:01.735059Z   \n",
      "3  Published        NaN      NaN  2024-06-21T03:54:28.477500Z   \n",
      "4  Published        NaN      NaN  2024-06-21T03:52:29.073154Z   \n",
      "\n",
      "             meta.date_created  \n",
      "0  2015-10-14T23:13:02.953992Z  \n",
      "1  2015-10-14T22:30:08.285506Z  \n",
      "2  2015-10-14T22:47:43.707700Z  \n",
      "3  2015-10-14T22:38:57.310766Z  \n",
      "4  2015-10-14T23:10:33.905566Z  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "# Combine all CSVs into a single DataFrame\n",
    "combined_df = combine_csvs_to_dataframe()\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('combined_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique author_ids: 89\n",
      "   author_id  docket_id\n",
      "0         48        232\n",
      "1         49        160\n",
      "2         51        199\n",
      "3         57         28\n",
      "4         58         18\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# Function to safely parse the opinions column\n",
    "def parse_opinions(opinions):\n",
    "    try:\n",
    "        # Use ast.literal_eval to convert string representation of list to actual list\n",
    "        parsed_opinions = ast.literal_eval(opinions)\n",
    "        if isinstance(parsed_opinions, list) and len(parsed_opinions) > 0:\n",
    "            return parsed_opinions[0].get('author_id', None)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "# Apply the function to extract author_id\n",
    "combined_df['author_id'] = combined_df['opinions'].apply(parse_opinions)\n",
    "\n",
    "# Convert author_id to numeric, safely handling non-numeric cases\n",
    "combined_df['author_id'] = pd.to_numeric(combined_df['author_id'], errors='coerce').astype('Int64')\n",
    "\n",
    "# Count of unique author_id\n",
    "unique_author_ids = combined_df['author_id'].nunique()\n",
    "print(f\"Total unique author_ids: {unique_author_ids}\")\n",
    "\n",
    "# Count of unique docket_id per author_id\n",
    "docket_id_counts = combined_df.groupby('author_id')['docket_id'].nunique().reset_index()\n",
    "print(docket_id_counts.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "docket_id_counts.to_csv('docket_id_counts.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "judges-conflicts",
   "language": "python",
   "name": "judges-conflicts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
